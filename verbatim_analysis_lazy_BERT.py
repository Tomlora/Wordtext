"""
Analyse de verbatims clients â€” Version haute volumÃ©trie (20M+ conversations).
Architecture LazyFrame-first : chaque Ã©tape ne collecte que ce dont elle a besoin.

OptimisÃ© avec :
    - Polars LazyFrame pour repousser les collectes au maximum
    - Jamais de .collect() sur les 20M lignes complÃ¨tes (sauf parsing string)
    - Sentiment vectorisÃ© via expressions Polars (regex Rust)
    - LDA sur Ã©chantillon reprÃ©sentatif
    - Comptages termes prÃ©-agrÃ©gÃ©s par streaming/chunks
    - RAM maÃ®trisÃ©e : on ne matÃ©rialise que des sous-ensembles

PrÃ©requis :
    pip install polars pyarrow numpy scikit-learn matplotlib seaborn

EntrÃ©e acceptÃ©e :
    - pl.LazyFrame  (recommandÃ© : scan_parquet, scan_csvâ€¦)
    - pl.DataFrame
    - pd.DataFrame
    Colonnes requises : date, conversation_id, verbatims (liste ou string repr)

Usage :
    # IdÃ©al : LazyFrame depuis un fichier
    lf = pl.scan_parquet("verbatims.parquet")
    analyzer = VerbatimAnalyzer(lf, date_col="date", verbatim_col="verbatims")
    report = analyzer.run_full_analysis()

    # Fonctionne aussi avec un DataFrame classique
    analyzer = VerbatimAnalyzer(df_pandas, ...)
"""

import re
import warnings
from collections import Counter
from datetime import date, timedelta
from typing import Optional, Union

import numpy as np
import polars as pl
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

warnings.filterwarnings("ignore")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NEGATIVE_LEXICON: set[str] = {
    "scandaleux", "honteux", "inadmissible", "inacceptable", "honte",
    "nul", "nulle", "catastrophe", "catastrophique", "horrible",
    "lamentable", "dÃ©plorable", "minable", "pitoyable",
    "panne", "bug", "bugge", "plante", "plantÃ©",
    "bloquÃ©", "bloquÃ©e", "erreur", "impossible",
    "hs", "coupure", "coupÃ©",
    "mÃ©content", "mÃ©contente", "dÃ©Ã§u", "dÃ©Ã§ue", "insatisfait",
    "insatisfaite", "plainte", "rÃ©clamation", "arnaque", "voleur",
    "voleurs", "menteur", "menteurs", "incompÃ©tent", "incompÃ©tente",
    "incompÃ©tents",
    "attente", "attend", "attends", "relance", "relancÃ©",
    "rÃ©silier", "rÃ©siliation", "rÃ©siliÃ©", "partir", "quitter",
    "concurrence", "concurrent",
    "surfacturation", "remboursement", "rembourser",
}

NEGATIVE_BIGRAMS: set[str] = {
    "marche pas", "fonctionne pas", "personne rÃ©pond", "trop cher",
    "hors service", "pure simple", "porter plainte", "trois semaines",
}

STOPWORDS_FR: set[str] = {
    "je", "tu", "il", "elle", "on", "nous", "vous", "ils", "elles",
    "le", "la", "les", "un", "une", "des", "de", "du", "au", "aux",
    "ce", "cette", "ces", "mon", "ma", "mes", "ton", "ta", "tes",
    "son", "sa", "ses", "notre", "votre", "leur", "leurs",
    "et", "ou", "mais", "donc", "car", "ni", "que", "qui", "dont",
    "oÃ¹", "en", "dans", "sur", "sous", "avec", "sans", "pour", "par",
    "ne", "pas", "plus", "trÃ¨s", "bien", "aussi", "comme", "tout",
    "Ãªtre", "avoir", "faire", "est", "sont", "suis", "ai", "a",
    "oui", "non", "merci", "bonjour", "bonsoir", "svp", "ok",
    "alors", "Ã§a", "cela", "lÃ ", "ici", "voilÃ ", "si", "se",
    "me", "te", "lui", "y", "Ã ", "mÃªme", "quand", "peu", "peut",
    "encore", "aprÃ¨s", "avant", "dÃ©jÃ ", "tous", "toute", "toutes",
    "Ã©tÃ©", "fait", "dit", "mis", "pris", "rien", "quelque",
}

# Regex patterns prÃ©-compilÃ©s
_UNIGRAM_PATTERN = r"\b(" + "|".join(re.escape(w) for w in NEGATIVE_LEXICON) + r")\b"
_BIGRAM_PATTERN = r"(" + "|".join(re.escape(b) for b in NEGATIVE_BIGRAMS) + r")"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPRESSIONS POLARS RÃ‰UTILISABLES (lazy-compatible)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _clean_expr(col: str = "verbatim") -> pl.Expr:
    """Expression Polars pour nettoyer un texte. Compatible lazy."""
    return (
        pl.col(col)
        .str.to_lowercase()
        .str.replace_all(r"http\S+", "")
        .str.replace_all(r"[^a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Ã§Å“Ã¦\s'-]", " ")
        .str.replace_all(r"\s+", " ")
        .str.strip_chars()
    )


def _neg_score_expr(col: str = "clean") -> pl.Expr:
    """Score de nÃ©gativitÃ© via regex. Compatible lazy."""
    return (
        (
            pl.col(col).str.count_matches(_UNIGRAM_PATTERN)
            + pl.col(col).str.count_matches(_BIGRAM_PATTERN)
        )
        .cast(pl.Float64)
        / pl.col(col).str.count_matches(r"\S+").cast(pl.Float64).clip(lower_bound=1)
    ).clip(upper_bound=1.0)


def _sentiment_exprs(col: str = "clean") -> list[pl.Expr]:
    """Retourne les expressions sentiment + is_negative. Compatible lazy."""
    return [
        _neg_score_expr(col).alias("neg_score"),
        (_neg_score_expr(col) > 0.15).alias("is_negative"),
    ]


def _parse_string_list_expr(col: str) -> pl.Expr:
    """
    Expression Polars pure pour parser une string repr de liste en List[Utf8].
    100% lazy, 0 Python, 0 collect.

    GÃ¨re les formats :
        '["aaa", "bbb", "ccc"]'
        "['aaa', 'bbb', 'ccc']"
        '["aaa", "bb bb", "ccc"]'  (espaces dans les Ã©lÃ©ments)

    StratÃ©gie :
        1. Strip les crochets extÃ©rieurs [ ]
        2. Strip les guillemets/apostrophes aux extrÃ©mitÃ©s de chaque Ã©lÃ©ment
        3. Split sur le sÃ©parateur  '", "'  ou  "', '"  (avec variantes d'espacement)
        4. Nettoyage final des guillemets rÃ©siduels sur chaque Ã©lÃ©ment
    """
    return (
        pl.col(col)
        .str.strip_chars()
        # 1. Retirer les crochets
        .str.strip_chars("[]")
        .str.strip_chars()
        # 2. Split sur les sÃ©parateurs entre Ã©lÃ©ments
        #    GÃ¨re : "aaa", "bbb"  ou  'aaa', 'bbb' ou mÃ©langes
        .str.split(", ")
        # 3. Nettoyer chaque Ã©lÃ©ment : retirer guillemets/apostrophes
        .list.eval(
            pl.element()
            .str.strip_chars()
            .str.strip_chars("'\"")
            .str.strip_chars()
        )
        # 4. Filtrer les Ã©lÃ©ments vides
        .list.eval(
            pl.element().filter(pl.element().str.len_chars() > 0)
        )
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANALYSEUR PRINCIPAL â€” ARCHITECTURE LAZYFRAME
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VerbatimAnalyzer:
    """
    Analyseur de verbatims clients â€” LazyFrame-first.

    Principe : self.lf est un LazyFrame prÃ©parÃ© (nettoyÃ©, avec colonne 'clean').
    Chaque mÃ©thode d'analyse collecte UNIQUEMENT les colonnes/lignes nÃ©cessaires.
    Jamais de .collect() sur l'intÃ©gralitÃ© des 20M lignes avec toutes les colonnes.

    Sentiment en 2 passes (si use_transformer=True) :
        Passe 1 â€” Lexique (rapide, 20M) : score basÃ© sur des mots-clÃ©s nÃ©gatifs
        Passe 2 â€” CamemBERT (prÃ©cis, Ã©chantillon) : sur les conversations ambiguÃ«s
                  (0.01 < neg_score < ambiguous_threshold), puis extrapolation

    Params:
        df                      : LazyFrame, DataFrame Polars, ou DataFrame Pandas
        date_col                : nom de la colonne date
        verbatim_col            : nom de la colonne contenant les listes de verbatims
        conversation_id_col     : nom de la colonne id de conversation
        z_threshold             : seuil de z-score pour dÃ©tecter un pic (dÃ©faut 2.0)
        growth_window           : fenÃªtre glissante en jours pour la croissance
        growth_factor           : multiplicateur pour un terme en forte croissance
        top_n_terms             : nombre de termes dans les rÃ©sultats
        lda_sample_size         : taille de l'Ã©chantillon pour la LDA
        chunk_size              : taille des chunks pour le comptage de termes
        use_transformer         : activer la passe 2 CamemBERT (dÃ©faut False)
        ambiguous_threshold     : seuil haut de la zone ambiguÃ« (dÃ©faut 0.30)
        transformer_sample_size : nb max de conversations ambiguÃ«s Ã  scorer (dÃ©faut 50K)
        transformer_batch_size  : taille de batch pour l'infÃ©rence (dÃ©faut 32)
    """

    def __init__(
        self,
        df: Union[pl.LazyFrame, pl.DataFrame, "pd.DataFrame"],
        date_col: str = "date",
        verbatim_col: str = "verbatims",
        conversation_id_col: str = "conversation_id",
        z_threshold: float = 2.0,
        growth_window: int = 7,
        growth_factor: float = 3.0,
        top_n_terms: int = 20,
        lda_sample_size: int = 200_000,
        chunk_size: int = 500_000,
        use_transformer: bool = False,
        ambiguous_threshold: float = 0.30,
        transformer_sample_size: int = 50_000,
        transformer_batch_size: int = 32,
    ):
        self.date_col = date_col
        self.verbatim_col = verbatim_col
        self.conversation_id_col = conversation_id_col
        self.z_threshold = z_threshold
        self.growth_window = growth_window
        self.growth_factor = growth_factor
        self.top_n_terms = top_n_terms
        self.lda_sample_size = lda_sample_size
        self.chunk_size = chunk_size
        self.use_transformer = use_transformer
        self.ambiguous_threshold = ambiguous_threshold
        self.transformer_sample_size = transformer_sample_size
        self.transformer_batch_size = transformer_batch_size

        # RÃ©sultats
        self.daily: Optional[pl.DataFrame] = None
        self.spike_details: list[dict] = []
        self.topics: list[dict] = []
        self.trending_terms = pl.DataFrame()
        self.novel_terms = pl.DataFrame()

        # PrÃ©paration â†’ LazyFrame
        self.lf = self._prepare(df)

        # Bornes de dates (collecte minimale)
        date_bounds = (
            self.lf.select(
                pl.col(self.date_col).min().alias("min_date"),
                pl.col(self.date_col).max().alias("max_date"),
                pl.count().alias("n_rows"),
            )
            .collect()
        )
        self.min_date: date = date_bounds["min_date"][0]
        self.max_date: date = date_bounds["max_date"][0]
        self.n_rows: int = date_bounds["n_rows"][0]

        print(f"âœ… {self.n_rows:,} conversations prÃªtes (LazyFrame)")

    # â”€â”€ PrÃ©paration â†’ LazyFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _prepare(self, df) -> pl.LazyFrame:
        """
        PrÃ©pare un LazyFrame nettoyÃ© avec colonne 'clean'.
        100% lazy dans tous les cas â€” jamais de collect() sur les 20M lignes.
        """
        print("â³ PrÃ©paration des donnÃ©es...")

        # â”€â”€ Conversion vers Polars LazyFrame â”€â”€
        import pandas as pd
        if isinstance(df, pd.DataFrame):
            print("   Conversion Pandas â†’ Polars...")
            df = pl.from_pandas(df)

        if isinstance(df, pl.DataFrame):
            lf = df.lazy()
        elif isinstance(df, pl.LazyFrame):
            lf = df
        else:
            raise TypeError(
                f"Type non supportÃ© : {type(df)}. "
                "Attendu : pl.LazyFrame, pl.DataFrame, ou pd.DataFrame"
            )

        # â”€â”€ DÃ©tection du type de colonne (collecte 1 seule ligne) â”€â”€
        sample_row = lf.select(self.verbatim_col).head(1).collect()
        col_dtype = sample_row[self.verbatim_col].dtype
        is_string_col = col_dtype in (pl.Utf8, pl.String)
        is_list_col = col_dtype.base_type() == pl.List

        if is_string_col:
            # Parsing string â†’ liste via expressions Polars pures (100% lazy)
            print("   Parsing des listes string (100% lazy, regex Polars)...")
            lf = lf.with_columns(
                _parse_string_list_expr(self.verbatim_col)
                .alias("verbatim_list")
            )
        elif is_list_col:
            lf = lf.with_columns(
                pl.col(self.verbatim_col).alias("verbatim_list")
            )
        else:
            # Colonne scalaire â†’ wrap en liste Ã  1 Ã©lÃ©ment
            lf = lf.with_columns(
                pl.col(self.verbatim_col)
                .cast(pl.Utf8)
                .map_elements(lambda x: [x], return_dtype=pl.List(pl.Utf8))
                .alias("verbatim_list")
            )

        # â”€â”€ ConcatÃ©nation + nettoyage (tout lazy) â”€â”€
        lf = (
            lf
            .with_columns(
                pl.col("verbatim_list").list.join(". ").alias("verbatim"),
                pl.col("verbatim_list").list.len().alias("n_messages"),
            )
            .with_columns(_clean_expr("verbatim").alias("clean"))
            .filter(pl.col("clean").str.len_chars() > 2)
            .with_columns(pl.col(self.date_col).cast(pl.Date))
        )

        mode = "100% lazy" if (is_string_col or is_list_col) else "lazy aprÃ¨s conversion"
        print(f"   âœ… PrÃ©paration {mode}")
        return lf

    # â”€â”€ 1. Sentiment & DÃ©tection de pics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _refine_with_transformer(self, daily: pl.DataFrame) -> pl.DataFrame:
        """
        Passe 2 : CamemBERT sur un Ã©chantillon de conversations ambiguÃ«s
        et nÃ©gatives (lexique), puis correction des stats journaliÃ¨res.

        Pipeline :
            1. Collecte les conversations avec 0.01 < neg_score < ambiguous_threshold
               (zone grise : le lexique ne sait pas trancher)
            2. Ã‰chantillonne transformer_sample_size conversations
            3. Passe CamemBERT dessus (CPU, par batch)
            4. Calcule le taux de correction (faux nÃ©gatifs rÃ©cupÃ©rÃ©s)
            5. Ajuste les comptages journaliers
        """
        try:
            from transformers import pipeline as hf_pipeline
        except ImportError:
            print("   âš ï¸  transformers et/ou torch non installÃ©s.")
            print("   â†’ pip install transformers torch")
            print("   â†’ Poursuite avec le lexique seul.")
            return daily

        print(f"â³ Passe 2 : CamemBERT sur conversations ambiguÃ«s...")
        print(f"   Seuil ambigu : 0.01 < neg_score < {self.ambiguous_threshold}")
        print(f"   Ã‰chantillon max : {self.transformer_sample_size:,}")

        # â”€â”€ Chargement du modÃ¨le â”€â”€
        print("   Chargement du modÃ¨le distilcamembert-base-sentiment...")
        classifier = hf_pipeline(
            "sentiment-analysis",
            model="cmarkea/distilcamembert-base-sentiment",
            truncation=True,
            max_length=512,
            device=-1,  # CPU
        )

        # â”€â”€ Collecte des ambigus (Ã©chantillon) â”€â”€
        ambiguous = (
            self.lf
            .with_columns(_neg_score_expr("clean").alias("neg_score"))
            .filter(
                (pl.col("neg_score") > 0.01)
                & (pl.col("neg_score") <= self.ambiguous_threshold)
            )
            .select(self.date_col, "clean", "neg_score")
            .collect()
        )
        n_ambiguous_total = ambiguous.height
        print(f"   {n_ambiguous_total:,} conversations ambiguÃ«s trouvÃ©es")

        if n_ambiguous_total == 0:
            print("   Aucune conversation ambiguÃ« â†’ pas de correction")
            return daily

        # Ã‰chantillonnage
        if n_ambiguous_total > self.transformer_sample_size:
            sample = ambiguous.sample(
                n=self.transformer_sample_size, seed=42
            )
        else:
            sample = ambiguous
        print(f"   Ã‰chantillon retenu : {sample.height:,} conversations")

        # â”€â”€ InfÃ©rence CamemBERT par batch â”€â”€
        texts = sample.get_column("clean").to_list()
        # Tronquer les textes trop longs pour Ã©viter les lenteurs
        texts = [t[:500] for t in texts]

        print(f"   InfÃ©rence CamemBERT ({sample.height:,} textes, "
              f"batch_size={self.transformer_batch_size})...")

        transformer_scores = []
        n_batches = (len(texts) + self.transformer_batch_size - 1) // self.transformer_batch_size

        for i in range(0, len(texts), self.transformer_batch_size):
            batch = texts[i:i + self.transformer_batch_size]
            results = classifier(batch)
            for r in results:
                # Le modÃ¨le retourne "1 star" Ã  "5 stars"
                # 1 star = trÃ¨s nÃ©gatif â†’ score 1.0
                # 5 stars = trÃ¨s positif â†’ score 0.0
                label = r["label"]
                stars = int(label.split()[0]) if label[0].isdigit() else 3
                neg = 1.0 - (stars - 1) / 4.0
                transformer_scores.append(neg)

            batch_num = i // self.transformer_batch_size + 1
            if batch_num % 50 == 0 or batch_num == n_batches:
                print(f"   ... batch {batch_num}/{n_batches}")

        # â”€â”€ Calcul du taux de correction â”€â”€
        sample = sample.with_columns(
            pl.Series(name="transformer_score", values=transformer_scores)
        ).with_columns(
            (pl.col("transformer_score") > 0.6).alias("transformer_negative")
        )

        # Taux de vrais nÃ©gatifs parmi les ambigus
        n_recovered = sample.filter(pl.col("transformer_negative")).height
        recovery_rate = n_recovered / sample.height if sample.height > 0 else 0.0
        print(f"   CamemBERT : {n_recovered}/{sample.height} ambigus "
              f"reclassÃ©s nÃ©gatifs ({recovery_rate:.1%})")

        # â”€â”€ Correction des stats journaliÃ¨res â”€â”€
        # Compter les ambigus par jour (sur le total, pas l'Ã©chantillon)
        ambiguous_daily = (
            ambiguous
            .group_by(self.date_col)
            .agg(pl.count().alias("ambiguous_count"))
        )

        daily = daily.join(ambiguous_daily, on=self.date_col, how="left")
        daily = daily.with_columns(
            pl.col("ambiguous_count").fill_null(0)
        )

        # Ajouter les nÃ©gatifs rÃ©cupÃ©rÃ©s (extrapolation du recovery_rate)
        daily = daily.with_columns(
            (
                pl.col("negative_count")
                + (pl.col("ambiguous_count") * recovery_rate).round(0).cast(pl.Int64)
            ).alias("negative_count_corrected")
        ).with_columns(
            (pl.col("negative_count_corrected") / pl.col("total_conversations"))
            .alias("negative_ratio_corrected")
        )

        # Recalculer le z-score sur le ratio corrigÃ©
        daily = daily.with_columns(
            pl.col("negative_ratio_corrected")
            .rolling_mean(window_size=30, min_periods=7)
            .alias("rolling_mean_corrected"),
            pl.col("negative_ratio_corrected")
            .rolling_std(window_size=30, min_periods=7)
            .alias("rolling_std_corrected"),
        ).with_columns(
            (
                (pl.col("negative_ratio_corrected") - pl.col("rolling_mean_corrected"))
                / pl.col("rolling_std_corrected").clip(lower_bound=1e-9)
            ).alias("z_score_corrected")
        ).with_columns(
            (pl.col("z_score_corrected") > self.z_threshold)
            .alias("is_spike_corrected")
        )

        # Stats de correction
        n_neg_before = daily["negative_count"].sum()
        n_neg_after = daily["negative_count_corrected"].sum()
        n_spikes_before = daily.filter(pl.col("is_spike")).height
        n_spikes_after = daily.filter(
            pl.col("is_spike_corrected").fill_null(False)
        ).height

        print(f"   ğŸ“Š Correction appliquÃ©e :")
        print(f"      NÃ©gatifs : {n_neg_before:,} â†’ {n_neg_after:,} "
              f"(+{n_neg_after - n_neg_before:,})")
        print(f"      Pics     : {n_spikes_before} â†’ {n_spikes_after}")

        self.transformer_recovery_rate = recovery_rate
        self.transformer_sample_stats = {
            "n_ambiguous_total": n_ambiguous_total,
            "n_sampled": sample.height,
            "n_recovered": n_recovered,
            "recovery_rate": recovery_rate,
        }

        return daily

    def detect_negative_spikes(self) -> pl.DataFrame:
        """
        DÃ©tecte les pics de nÃ©gativitÃ©.

        Passe 1 : lexique (sur les 20M, via agrÃ©gation lazy)
        Passe 2 (optionnelle) : CamemBERT sur les ambigus pour corriger
        """
        print("â³ Passe 1 : sentiment lexique + dÃ©tection des pics...")

        # AgrÃ©gation jour directement en lazy
        daily = (
            self.lf
            .with_columns(*_sentiment_exprs("clean"))
            .group_by(self.date_col)
            .agg(
                pl.count().alias("total_conversations"),
                pl.col("is_negative").sum().alias("negative_count"),
                pl.col("neg_score").mean().alias("mean_neg_score"),
            )
            .sort(self.date_col)
            .collect()
        )

        daily = daily.with_columns(
            (pl.col("negative_count") / pl.col("total_conversations"))
            .alias("negative_ratio")
        )

        # Z-score glissant
        daily = daily.with_columns(
            pl.col("negative_ratio")
            .rolling_mean(window_size=30, min_periods=7)
            .alias("rolling_mean"),
            pl.col("negative_ratio")
            .rolling_std(window_size=30, min_periods=7)
            .alias("rolling_std"),
        ).with_columns(
            (
                (pl.col("negative_ratio") - pl.col("rolling_mean"))
                / pl.col("rolling_std").clip(lower_bound=1e-9)
            ).alias("z_score")
        ).with_columns(
            (pl.col("z_score") > self.z_threshold).alias("is_spike")
        )

        n_neg = daily["negative_count"].sum()
        print(f"   {n_neg:,} conversations nÃ©gatives (lexique) "
              f"({n_neg / self.n_rows * 100:.1f}%)")

        # â”€â”€ Passe 2 : CamemBERT (optionnelle) â”€â”€
        if self.use_transformer:
            daily = self._refine_with_transformer(daily)

        # DÃ©terminer quelle colonne de spike utiliser
        has_corrected = "is_spike_corrected" in daily.columns
        spike_col = "is_spike_corrected" if has_corrected else "is_spike"

        # DÃ©tails des pics : collecte ciblÃ©e par date
        spike_dates = (
            daily.filter(
                pl.col(spike_col).fill_null(False)
            )
            .get_column(self.date_col)
            .to_list()
        )

        spike_details = []
        for d in spike_dates:
            day_neg = (
                self.lf
                .with_columns(*_sentiment_exprs("clean"))
                .filter(
                    (pl.col(self.date_col) == d) & pl.col("is_negative")
                )
                .select("clean", "verbatim")
                .collect()
            )
            all_tokens = []
            for text in day_neg.get_column("clean").to_list():
                words = text.split()
                all_tokens.extend(
                    w for w in words if w not in STOPWORDS_FR and len(w) > 2
                )
            top_terms = Counter(all_tokens).most_common(10)
            samples = day_neg.get_column("verbatim").head(5).to_list()
            spike_details.append({
                "date": d,
                "top_negative_terms": top_terms,
                "sample_verbatims": samples,
            })

        self.daily = daily
        self.spike_details = spike_details
        print(f"âœ… {len(spike_dates)} pic(s) de nÃ©gativitÃ© dÃ©tectÃ©(s)")
        return daily

    # â”€â”€ 2. Sujets Ã©mergents (LDA sur Ã©chantillon) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def detect_emerging_topics(
        self, n_topics: int = 10, recent_days: int = 14
    ) -> dict:
        """
        LDA sur Ã©chantillon. Collecte uniquement l'Ã©chantillon (~200K lignes),
        pas les 20M.
        """
        print(f"â³ DÃ©tection des sujets Ã©mergents "
              f"(LDA sur Ã©chantillon de {self.lda_sample_size:,})...")

        cutoff = self.max_date - timedelta(days=recent_days)

        # Compter les volumes pour calculer les proportions (collect minimal)
        counts = (
            self.lf
            .select(
                (pl.col(self.date_col) >= cutoff).alias("is_recent")
            )
            .group_by("is_recent")
            .agg(pl.count().alias("n"))
            .collect()
        )
        n_recent_total = counts.filter(pl.col("is_recent"))["n"][0] if counts.filter(pl.col("is_recent")).height > 0 else 0
        n_history_total = counts.filter(~pl.col("is_recent"))["n"][0] if counts.filter(~pl.col("is_recent")).height > 0 else 0

        recent_ratio = n_recent_total / (n_recent_total + n_history_total)
        n_recent_sample = max(int(self.lda_sample_size * recent_ratio), 10_000)
        n_history_sample = self.lda_sample_size - n_recent_sample

        # Collecte UNIQUEMENT l'Ã©chantillon (colonne clean seulement)
        recent_sample = (
            self.lf
            .filter(pl.col(self.date_col) >= cutoff)
            .select("clean")
            .collect()
            .sample(n=min(n_recent_sample, n_recent_total), seed=42)
        )
        history_sample = (
            self.lf
            .filter(pl.col(self.date_col) < cutoff)
            .select("clean")
            .collect()
            .sample(n=min(n_history_sample, n_history_total), seed=42)
        )

        print(f"   Ã‰chantillon : {history_sample.height:,} historique + "
              f"{recent_sample.height:,} rÃ©cent")

        is_recent = [False] * history_sample.height + [True] * recent_sample.height
        all_texts = (
            history_sample.get_column("clean").to_list()
            + recent_sample.get_column("clean").to_list()
        )

        # Vectorisation + LDA
        vectorizer = CountVectorizer(
            max_features=3000, min_df=10, max_df=0.8,
            stop_words=list(STOPWORDS_FR), ngram_range=(1, 2),
        )
        X = vectorizer.fit_transform(all_texts)
        feature_names = vectorizer.get_feature_names_out()

        lda = LatentDirichletAllocation(
            n_components=n_topics, random_state=42, max_iter=15, n_jobs=-1,
        )
        doc_topics = lda.fit_transform(X)

        is_recent_arr = np.array(is_recent)
        recent_dist = doc_topics[is_recent_arr].mean(axis=0)
        history_dist = doc_topics[~is_recent_arr].mean(axis=0)

        with np.errstate(divide="ignore", invalid="ignore"):
            emergence_ratio = np.where(
                history_dist > 0.001,
                recent_dist / history_dist,
                recent_dist * 100,
            )

        topics = []
        for idx in range(n_topics):
            top_word_indices = lda.components_[idx].argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_word_indices]
            topics.append({
                "topic_id": idx,
                "top_words": top_words,
                "recent_weight": round(float(recent_dist[idx]), 4),
                "history_weight": round(float(history_dist[idx]), 4),
                "emergence_ratio": round(float(emergence_ratio[idx]), 2),
            })

        topics = sorted(topics, key=lambda x: x["emergence_ratio"], reverse=True)
        self.topics = topics
        emerging = [t for t in topics if t["emergence_ratio"] > 1.5]
        print(f"âœ… {len(emerging)} sujet(s) Ã©mergent(s) dÃ©tectÃ©(s) (ratio > 1.5x)")
        return {"all_topics": topics, "emerging": emerging}

    # â”€â”€ 3. Termes en forte croissance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def detect_trending_terms(self) -> pl.DataFrame:
        """
        DÃ©tecte les termes en forte croissance.
        Collecte par chunks (seulement 2 colonnes : date + clean).
        """
        print("â³ DÃ©tection des termes en forte croissance...")
        print("   Comptage des termes par jour (par chunks)...")

        # On ne collecte que date + clean, jamais le reste
        minimal_lf = self.lf.select(self.date_col, "clean")

        term_day_counts: Counter = Counter()
        n = self.n_rows

        # Streaming par chunks via slice sur le LazyFrame
        for start in range(0, n, self.chunk_size):
            chunk_size = min(self.chunk_size, n - start)
            chunk = minimal_lf.slice(start, chunk_size).collect()

            chunk_texts = chunk.get_column("clean").to_list()
            chunk_dates = chunk.get_column(self.date_col).to_list()

            for text, d in zip(chunk_texts, chunk_dates):
                words = text.split()
                seen = set()
                for w in words:
                    if w not in STOPWORDS_FR and len(w) > 2 and w not in seen:
                        term_day_counts[(str(d), w)] += 1
                        seen.add(w)

            processed = start + chunk_size
            if processed % (self.chunk_size * 5) == 0 and processed > 0:
                print(f"   ... {processed:,}/{n:,} conversations")

        print(f"   {len(term_day_counts):,} paires (terme, jour) comptÃ©es")

        # AgrÃ©gation en Polars
        records = [
            {"date": k[0], "term": k[1], "count": v}
            for k, v in term_day_counts.items()
        ]
        term_df = pl.DataFrame(records).with_columns(
            pl.col("date").str.to_date()
        )

        max_date = term_df.get_column("date").max()
        recent_start = max_date - timedelta(days=self.growth_window)
        baseline_end = recent_start
        baseline_start = baseline_end - timedelta(days=30)

        recent_avg = (
            term_df.lazy()
            .filter(pl.col("date") >= recent_start)
            .group_by("term")
            .agg(
                (pl.col("count").sum() / self.growth_window)
                .alias("current_avg_daily")
            )
        )

        baseline_avg = (
            term_df.lazy()
            .filter(
                (pl.col("date") >= baseline_start)
                & (pl.col("date") < baseline_end)
            )
            .group_by("term")
            .agg(
                (pl.col("count").sum() / 30.0).alias("baseline_avg_daily")
            )
        )

        trending = (
            recent_avg
            .join(baseline_avg, on="term", how="left")
            .with_columns(pl.col("baseline_avg_daily").fill_null(0.0))
            .with_columns(
                pl.when(pl.col("baseline_avg_daily") > 0.5)
                .then(pl.col("current_avg_daily") / pl.col("baseline_avg_daily"))
                .otherwise(pl.col("current_avg_daily") * 10)
                .alias("growth_ratio")
            )
            .filter(
                (pl.col("growth_ratio") >= self.growth_factor)
                & (pl.col("current_avg_daily") >= 2)
            )
            .sort("growth_ratio", descending=True)
            .head(self.top_n_terms)
            .with_columns(
                pl.col("current_avg_daily").round(2),
                pl.col("baseline_avg_daily").round(2),
                pl.col("growth_ratio").round(2),
            )
            .collect()
        )

        self.trending_terms = trending
        print(f"âœ… {trending.height} terme(s) en forte croissance "
              f"(x{self.growth_factor}+)")
        return trending

    # â”€â”€ 4. Termes inhabituels (nouveaux) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def detect_novel_terms(
        self, recent_days: int = 7, min_count: int = 20
    ) -> pl.DataFrame:
        """
        DÃ©tecte les termes nouveaux. Collecte par chunks (2 colonnes seulement).
        """
        print("â³ DÃ©tection des termes inhabituels...")

        cutoff = self.max_date - timedelta(days=recent_days)
        total_hist_days = (cutoff - self.min_date).days or 1

        minimal_lf = self.lf.select(self.date_col, "clean")

        recent_counts: Counter = Counter()
        history_counts: Counter = Counter()

        for start in range(0, self.n_rows, self.chunk_size):
            chunk_size = min(self.chunk_size, self.n_rows - start)
            chunk = minimal_lf.slice(start, chunk_size).collect()

            chunk_texts = chunk.get_column("clean").to_list()
            chunk_dates = chunk.get_column(self.date_col).to_list()

            for text, d in zip(chunk_texts, chunk_dates):
                words = set(text.split())
                tokens = {w for w in words if w not in STOPWORDS_FR and len(w) > 2}
                if d >= cutoff:
                    recent_counts.update(tokens)
                else:
                    history_counts.update(tokens)

        novel = []
        for term, count in recent_counts.items():
            if count < min_count:
                continue
            hist = history_counts.get(term, 0)
            hist_daily = hist / total_hist_days
            recent_daily = count / recent_days

            if hist_daily < 1.0:
                novel.append({
                    "term": term,
                    "recent_count": count,
                    "recent_daily_avg": round(recent_daily, 2),
                    "history_total": hist,
                    "history_daily_avg": round(hist_daily, 2),
                    "novelty": "nouveau" if hist == 0 else "quasi_nouveau",
                })

        novel_df = (
            pl.DataFrame(novel)
            .sort("recent_count", descending=True)
            .head(self.top_n_terms)
        ) if novel else pl.DataFrame()

        self.novel_terms = novel_df
        print(f"âœ… {novel_df.height} terme(s) inhabituels dÃ©tectÃ©(s)")
        return novel_df

    # â”€â”€ 5. Visualisations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def plot_dashboard(self, save_path: Optional[str] = None):
        """Dashboard rÃ©capitulatif."""
        fig, axes = plt.subplots(3, 1, figsize=(16, 14), constrained_layout=True)
        fig.suptitle(
            "ğŸ“Š Dashboard Analyse Verbatims Clients",
            fontsize=16, fontweight="bold",
        )

        if self.daily is not None:
            ax1 = axes[0]
            ax1.set_title("Volume de conversations et pics de nÃ©gativitÃ©")
            daily_pd = self.daily.with_columns(
                pl.col("is_spike").fill_null(False)
            ).to_pandas()
            ax1.bar(
                daily_pd[self.date_col], daily_pd["total_conversations"],
                color="#c8d6e5", alpha=0.7, label="Total conversations",
            )
            ax1b = ax1.twinx()

            # Ratio nÃ©gatif (lexique)
            ax1b.plot(
                daily_pd[self.date_col], daily_pd["negative_ratio"],
                color="#e74c3c", linewidth=1.2, alpha=0.5,
                label="Ratio nÃ©gatif (lexique)",
            )

            # Si passe 2 CamemBERT : afficher le ratio corrigÃ©
            has_corrected = "negative_ratio_corrected" in daily_pd.columns
            if has_corrected:
                ax1b.plot(
                    daily_pd[self.date_col],
                    daily_pd["negative_ratio_corrected"],
                    color="#c0392b", linewidth=2,
                    label="Ratio nÃ©gatif (corrigÃ© CamemBERT)",
                )
                spike_col = "is_spike_corrected"
                ratio_col = "negative_ratio_corrected"
            else:
                spike_col = "is_spike"
                ratio_col = "negative_ratio"

            spikes = daily_pd[daily_pd[spike_col].fillna(False)]
            ax1b.scatter(
                spikes[self.date_col], spikes[ratio_col],
                color="red", s=80, zorder=5, marker="^", label="Pic dÃ©tectÃ©",
            )
            ax1.set_ylabel("Volume")
            ax1b.set_ylabel("Ratio nÃ©gatif", color="#e74c3c")
            ax1.legend(loc="upper left")
            ax1b.legend(loc="upper right")
            ax1.xaxis.set_major_formatter(mdates.DateFormatter("%d/%m"))

        ax2 = axes[1]
        if self.trending_terms.height > 0:
            data = self.trending_terms.head(15).to_pandas()
            colors = plt.cm.Reds(np.linspace(0.4, 0.9, len(data)))[::-1]
            ax2.barh(data["term"], data["growth_ratio"], color=colors)
            ax2.set_xlabel("Ratio de croissance")
            ax2.set_title("Termes en forte croissance (top 15)")
            ax2.invert_yaxis()
        else:
            ax2.text(0.5, 0.5, "Pas de donnÃ©es", ha="center", va="center")
            ax2.set_title("Termes en forte croissance")

        ax3 = axes[2]
        if self.novel_terms.height > 0:
            data = self.novel_terms.head(15).to_pandas()
            colors = [
                "#e67e22" if n == "nouveau" else "#f39c12"
                for n in data["novelty"]
            ]
            ax3.barh(data["term"], data["recent_count"], color=colors)
            ax3.set_xlabel("Occurrences rÃ©centes")
            ax3.set_title("Termes inhabituels / nouveaux (top 15)")
            ax3.invert_yaxis()
        else:
            ax3.text(0.5, 0.5, "Pas de donnÃ©es", ha="center", va="center")
            ax3.set_title("Termes inhabituels / nouveaux")

        if save_path:
            fig.savefig(save_path, dpi=150, bbox_inches="tight")
            print(f"ğŸ“ Dashboard sauvegardÃ© : {save_path}")

        plt.close(fig)
        return fig

    # â”€â”€ 6. Export Excel avec phrases exemples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def export_to_excel(
        self,
        path: str = "analyse_termes.xlsx",
        max_samples_per_term: int = 20,
    ) -> str:
        """
        Exporte les termes en forte croissance et les termes inhabituels
        dans un Excel avec les conversations exemples.

        Pour chaque terme, on collecte (en lazy) un Ã©chantillon de conversations
        contenant ce terme, sans jamais charger les 20M en mÃ©moire.

        Params:
            path                : chemin du fichier Excel de sortie
            max_samples_per_term: nombre max de conversations exemples par terme
        """
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side

        print(f"â³ Export Excel avec conversations exemples...")

        wb = Workbook()

        # Styles
        header_font = Font(name="Arial", bold=True, color="FFFFFF", size=11)
        header_fill_red = PatternFill("solid", fgColor="C0392B")
        header_fill_orange = PatternFill("solid", fgColor="E67E22")
        header_fill_grey = PatternFill("solid", fgColor="7F8C8D")
        term_font = Font(name="Arial", bold=True, size=10, color="2C3E50")
        term_fill = PatternFill("solid", fgColor="FADBD8")
        term_fill_novel = PatternFill("solid", fgColor="FAE5D3")
        data_font = Font(name="Arial", size=10)
        wrap_align = Alignment(wrap_text=True, vertical="top")
        center_align = Alignment(horizontal="center", vertical="center")
        thin_border = Border(
            bottom=Side(style="thin", color="D5D8DC"),
        )

        def _write_headers(ws, headers, fill):
            for col_idx, h in enumerate(headers, 1):
                cell = ws.cell(row=1, column=col_idx, value=h)
                cell.font = header_font
                cell.fill = fill
                cell.alignment = center_align
            ws.freeze_panes = "A2"

        def _fetch_samples(term: str, n: int) -> list[dict]:
            """Collecte en lazy les conversations les plus rÃ©centes contenant le terme."""
            samples = (
                self.lf
                .filter(pl.col("clean").str.contains(rf"\b{re.escape(term)}\b"))
                .select(
                    pl.col(self.date_col),
                    pl.col(self.conversation_id_col),
                    pl.col("verbatim"),
                )
                .sort(self.date_col, descending=True)
                .head(n)
                .collect()
            )
            return samples.to_dicts()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Onglet 1 : Termes en forte croissance
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ws1 = wb.active
        ws1.title = "Termes en croissance"
        _write_headers(ws1, [
            "Terme", "Croissance (x)", "Moy. actuelle/jour",
            "Moy. baseline/jour", "Date", "ID conversation",
            "Conversation complÃ¨te",
        ], header_fill_red)

        ws1.column_dimensions["A"].width = 20
        ws1.column_dimensions["B"].width = 14
        ws1.column_dimensions["C"].width = 16
        ws1.column_dimensions["D"].width = 16
        ws1.column_dimensions["E"].width = 13
        ws1.column_dimensions["F"].width = 18
        ws1.column_dimensions["G"].width = 80

        row = 2
        if self.trending_terms.height > 0:
            for term_row in self.trending_terms.iter_rows(named=True):
                term = term_row["term"]
                samples = _fetch_samples(term, max_samples_per_term)
                start_row = row

                if not samples:
                    # Ligne terme sans exemples
                    ws1.cell(row=row, column=1, value=term).font = term_font
                    ws1.cell(row=row, column=2, value=term_row["growth_ratio"]).font = data_font
                    ws1.cell(row=row, column=3, value=term_row["current_avg_daily"]).font = data_font
                    ws1.cell(row=row, column=4, value=term_row["baseline_avg_daily"]).font = data_font
                    row += 1
                else:
                    for i, s in enumerate(samples):
                        if i == 0:
                            ws1.cell(row=row, column=1, value=term).font = term_font
                            ws1.cell(row=row, column=2, value=term_row["growth_ratio"]).font = data_font
                            ws1.cell(row=row, column=3, value=term_row["current_avg_daily"]).font = data_font
                            ws1.cell(row=row, column=4, value=term_row["baseline_avg_daily"]).font = data_font
                        c_date = ws1.cell(row=row, column=5, value=str(s[self.date_col]))
                        c_date.font = data_font
                        c_id = ws1.cell(row=row, column=6, value=s[self.conversation_id_col])
                        c_id.font = data_font
                        c_verb = ws1.cell(row=row, column=7, value=s["verbatim"])
                        c_verb.font = data_font
                        c_verb.alignment = wrap_align
                        row += 1

                # Colorer le bloc du terme
                for r in range(start_row, row):
                    ws1.cell(row=r, column=1).fill = term_fill
                    for c in range(1, 8):
                        ws1.cell(row=r, column=c).border = thin_border

                # Ligne vide de sÃ©paration
                row += 1

            print(f"   âœ… Onglet 'Termes en croissance' : "
                  f"{self.trending_terms.height} termes")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Onglet 2 : Termes inhabituels / nouveaux
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ws2 = wb.create_sheet("Termes inhabituels")
        _write_headers(ws2, [
            "Terme", "Type", "Occurrences rÃ©centes",
            "Moy. rÃ©cente/jour", "Historique total",
            "Date", "ID conversation", "Conversation complÃ¨te",
        ], header_fill_orange)

        ws2.column_dimensions["A"].width = 20
        ws2.column_dimensions["B"].width = 16
        ws2.column_dimensions["C"].width = 18
        ws2.column_dimensions["D"].width = 16
        ws2.column_dimensions["E"].width = 16
        ws2.column_dimensions["F"].width = 13
        ws2.column_dimensions["G"].width = 18
        ws2.column_dimensions["H"].width = 80

        row = 2
        if self.novel_terms.height > 0:
            for term_row in self.novel_terms.iter_rows(named=True):
                term = term_row["term"]
                samples = _fetch_samples(term, max_samples_per_term)
                start_row = row

                if not samples:
                    ws2.cell(row=row, column=1, value=term).font = term_font
                    ws2.cell(row=row, column=2, value=term_row["novelty"]).font = data_font
                    ws2.cell(row=row, column=3, value=term_row["recent_count"]).font = data_font
                    ws2.cell(row=row, column=4, value=term_row["recent_daily_avg"]).font = data_font
                    ws2.cell(row=row, column=5, value=term_row["history_total"]).font = data_font
                    row += 1
                else:
                    for i, s in enumerate(samples):
                        if i == 0:
                            ws2.cell(row=row, column=1, value=term).font = term_font
                            ws2.cell(row=row, column=2, value=term_row["novelty"]).font = data_font
                            ws2.cell(row=row, column=3, value=term_row["recent_count"]).font = data_font
                            ws2.cell(row=row, column=4, value=term_row["recent_daily_avg"]).font = data_font
                            ws2.cell(row=row, column=5, value=term_row["history_total"]).font = data_font
                        c_date = ws2.cell(row=row, column=6, value=str(s[self.date_col]))
                        c_date.font = data_font
                        c_id = ws2.cell(row=row, column=7, value=s[self.conversation_id_col])
                        c_id.font = data_font
                        c_verb = ws2.cell(row=row, column=8, value=s["verbatim"])
                        c_verb.font = data_font
                        c_verb.alignment = wrap_align
                        row += 1

                for r in range(start_row, row):
                    ws2.cell(row=r, column=1).fill = term_fill_novel
                    for c in range(1, 9):
                        ws2.cell(row=r, column=c).border = thin_border

                row += 1

            print(f"   âœ… Onglet 'Termes inhabituels' : "
                  f"{self.novel_terms.height} termes")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Onglet 3 : RÃ©sumÃ©
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ws3 = wb.create_sheet("RÃ©sumÃ©")
        _write_headers(ws3, ["MÃ©trique", "Valeur"], header_fill_grey)
        ws3.column_dimensions["A"].width = 35
        ws3.column_dimensions["B"].width = 25

        summary = [
            ("PÃ©riode analysÃ©e", f"{self.min_date} â†’ {self.max_date}"),
            ("Nombre total de conversations", f"{self.n_rows:,}"),
            ("Pics de nÃ©gativitÃ© dÃ©tectÃ©s", str(len(self.spike_details))),
            ("Termes en forte croissance", str(self.trending_terms.height)),
            ("Termes inhabituels dÃ©tectÃ©s", str(self.novel_terms.height)),
        ]
        for i, (metric, val) in enumerate(summary, 2):
            ws3.cell(row=i, column=1, value=metric).font = Font(name="Arial", bold=True, size=10)
            ws3.cell(row=i, column=2, value=val).font = data_font

        # DÃ©placer RÃ©sumÃ© en premier
        wb.move_sheet("RÃ©sumÃ©", offset=-2)

        wb.save(path)
        print(f"ğŸ“ Excel sauvegardÃ© : {path}")
        return path

    def run_full_analysis(
        self,
        save_dashboard: Optional[str] = None,
        save_excel: Optional[str] = None,
        max_samples_per_term: int = 20,
    ) -> dict:
        """Lance l'analyse complÃ¨te."""
        print("=" * 60)
        print("ğŸ” ANALYSE COMPLÃˆTE DES VERBATIMS CLIENTS")
        print("=" * 60)
        print(f"ğŸ“… PÃ©riode : {self.min_date} â†’ {self.max_date}")
        print(f"ğŸ“ Nombre de conversations : {self.n_rows:,}")
        print()

        self.detect_negative_spikes()
        print()

        topics_result = self.detect_emerging_topics()
        print()

        trending = self.detect_trending_terms()
        print()

        novel = self.detect_novel_terms()
        print()

        fig = self.plot_dashboard(save_path=save_dashboard)

        # 6. Export Excel
        if save_excel:
            self.export_to_excel(
                path=save_excel,
                max_samples_per_term=max_samples_per_term,
            )
            print()

        # Rapport console
        print("=" * 60)
        print("ğŸ“‹ RÃ‰SUMÃ‰")
        print("=" * 60)

        if self.spike_details:
            print("\nğŸ”´ PICS DE NÃ‰GATIVITÃ‰ :")
            for sp in self.spike_details[:5]:
                terms_str = ", ".join(
                    f"{t[0]} ({t[1]})" for t in sp["top_negative_terms"][:5]
                )
                print(f"  ğŸ“… {sp['date']} â€” Termes : {terms_str}")
                for v in sp["sample_verbatims"][:2]:
                    print(f"     ğŸ’¬ \"{str(v)[:100]}\"")

        if topics_result["emerging"]:
            print("\nğŸŸ  SUJETS Ã‰MERGENTS :")
            for t in topics_result["emerging"][:5]:
                words = ", ".join(t["top_words"][:5])
                print(f"  Topic #{t['topic_id']} (x{t['emergence_ratio']}) : {words}")

        if trending.height > 0:
            print("\nğŸŸ¡ TERMES EN FORTE CROISSANCE :")
            for row in trending.head(10).iter_rows(named=True):
                print(
                    f"  ğŸ“ˆ {row['term']} : x{row['growth_ratio']} "
                    f"({row['baseline_avg_daily']}/j â†’ {row['current_avg_daily']}/j)"
                )

        if novel.height > 0:
            print("\nğŸ†• TERMES INHABITUELS / NOUVEAUX :")
            for row in novel.head(10).iter_rows(named=True):
                tag = "ğŸ†•" if row["novelty"] == "nouveau" else "âš ï¸"
                print(
                    f"  {tag} {row['term']} : {row['recent_count']} "
                    f"occurrences ({row['novelty']})"
                )

        print("\n" + "=" * 60)

        return {
            "daily_stats": self.daily,
            "spike_details": self.spike_details,
            "topics": topics_result,
            "trending_terms": trending,
            "novel_terms": novel,
            "figure": fig,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DÃ‰MO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import time
    from datetime import date as pydate

    np.random.seed(42)
    dates = pl.date_range(
        pydate(2024, 6, 1), pydate(2025, 2, 25), eager=True
    ).to_list()

    normal_conversations = [
        ["Bonjour je souhaite modifier mon offre", "oui celle en cours", "d'accord merci"],
        ["Quand sera traitÃ©e ma demande", "celle du mois dernier", "ok je patiente"],
        ["Je veux des informations sur mon contrat", "oui c'est bien Ã§a", "merci"],
        ["Pouvez-vous me rappeler", "demain matin si possible", "parfait"],
        ["J'aimerais changer mon mot de passe", "oui je confirme", "c'est bon merci"],
        ["Comment accÃ©der Ã  mon espace client", "je ne trouve pas le lien"],
        ["Je voudrais un conseiller", "pour parler de ma facture", "oui"],
    ]
    negative_conversations = [
        ["C'est scandaleux personne ne me rÃ©pond", "Ã§a fait des jours que j'attends",
         "je vais porter plainte"],
        ["Ã‡a fait trois semaines que j'attends", "toujours rien", "c'est inadmissible"],
        ["Service incompÃ©tent je vais rÃ©silier", "j'en ai assez",
         "passez-moi un responsable"],
        ["Panne depuis ce matin rien ne fonctionne", "toujours pas rÃ©tabli",
         "c'est la troisiÃ¨me fois ce mois"],
        ["Bug sur l'application impossible de se connecter", "j'ai tout essayÃ©",
         "Ã§a ne marche toujours pas"],
        ["Je suis trÃ¨s dÃ©Ã§u de votre service", "la qualitÃ© a vraiment baissÃ©"],
        ["Arnaque pure et simple remboursez-moi", "je n'ai jamais demandÃ© Ã§a"],
        ["Votre service est lamentable", "je vais aller voir la concurrence"],
        ["Je vais aller chez le concurrent", "vous Ãªtes trop cher",
         "j'ai reÃ§u une meilleure offre"],
        ["Surfacturation sur ma derniÃ¨re facture", "je veux un remboursement immÃ©diat"],
    ]
    emerging_conversations = [
        ["ProblÃ¨me avec la nouvelle mise Ã  jour fibre", "rien ne marche depuis",
         "quand est-ce que Ã§a sera rÃ©parÃ©"],
        ["Depuis la migration fibre plus rien ne marche", "internet coupÃ©",
         "c'est inadmissible"],
        ["Migration fibre catastrophique", "dÃ©bit ridicule", "je veux rÃ©silier"],
    ]

    conv_id = 0
    all_dates, all_ids, all_verbatims = [], [], []

    for d in dates:
        n = np.random.randint(50, 200)
        n_normal = int(n * 0.75)
        n_negative = n - n_normal

        for _ in range(n_normal):
            conv = normal_conversations[np.random.randint(len(normal_conversations))]
            all_dates.append(d)
            all_ids.append(f"conv_{conv_id:06d}")
            all_verbatims.append(str(conv))
            conv_id += 1

        for _ in range(n_negative):
            conv = negative_conversations[np.random.randint(len(negative_conversations))]
            all_dates.append(d)
            all_ids.append(f"conv_{conv_id:06d}")
            all_verbatims.append(str(conv))
            conv_id += 1

        if d == pydate(2025, 1, 15):
            for _ in range(100):
                conv = negative_conversations[np.random.randint(len(negative_conversations))]
                all_dates.append(d)
                all_ids.append(f"conv_{conv_id:06d}")
                all_verbatims.append(str(conv))
                conv_id += 1

        if d >= pydate(2025, 2, 12):
            for _ in range(30):
                conv = emerging_conversations[np.random.randint(len(emerging_conversations))]
                all_dates.append(d)
                all_ids.append(f"conv_{conv_id:06d}")
                all_verbatims.append(str(conv))
                conv_id += 1

    df_demo = pl.DataFrame({
        "date": all_dates,
        "conversation_id": all_ids,
        "verbatims": all_verbatims,
    })

    print(f"ğŸ“Š Dataset de dÃ©mo : {df_demo.height:,} conversations")
    print(f"   (en production : passer un LazyFrame via pl.scan_parquet())\n")

    start_time = time.time()

    analyzer = VerbatimAnalyzer(
        df_demo,
        date_col="date",
        verbatim_col="verbatims",
        conversation_id_col="conversation_id",
        z_threshold=2.0,
        growth_window=7,
        growth_factor=3.0,
        lda_sample_size=50_000,
    )
    report = analyzer.run_full_analysis(
        save_dashboard="dashboard_verbatims.png",
        save_excel="analyse_termes.xlsx",
        max_samples_per_term=10,
    )

    elapsed = time.time() - start_time
    print(f"\nâ±ï¸  Temps total : {elapsed:.1f}s")
